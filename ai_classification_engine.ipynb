{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a5c982",
   "metadata": {},
   "source": [
    "# AI Data Classification Engine (Batch, Azure/OpenAI)\n",
    "\n",
    "This notebook contains a **reusable classification pipeline** that:\n",
    "- Reads data from an Excel/CSV file\n",
    "- Sends rows in **batches** to Azure OpenAI or OpenAI\n",
    "- Applies **per-column predefined categories** (optional)\n",
    "- Generates **categorical + freetext outputs**\n",
    "- Writes the results back to Excel\n",
    "\n",
    "Each code block has instructions explaining how to use or modify it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2fcdf3",
   "metadata": {},
   "source": [
    "## 1. Imports and JSON helper\n",
    "Run this cell first.\n",
    "\n",
    "It imports all required libraries and defines a helper function to safely parse JSON returned by the model (handles minor formatting issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional, Set\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "\n",
    "\n",
    "def clean_and_parse_json(raw: str) -> dict:\n",
    "    \"\"\"\n",
    "    Clean common LLM formatting (e.g., ```json fences) and parse JSON.\n",
    "    Raises ValueError if parsing still fails.\n",
    "    \"\"\"\n",
    "    text = raw.strip()\n",
    "\n",
    "    # Remove markdown-style code fences if present\n",
    "    if text.startswith(\"```\"):\n",
    "        lines = text.splitlines()\n",
    "        # Drop first line (``` or ```json)\n",
    "        if lines and lines[0].startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        # Drop last line if it's just ```\n",
    "        if lines and lines[-1].strip().startswith(\"```\"):\n",
    "            lines = lines[:-1]\n",
    "        text = \"\\n\".join(lines).strip()\n",
    "\n",
    "    # Try direct parse first\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to salvage: cut at the last closing brace\n",
    "        last_brace = text.rfind(\"}\")\n",
    "        if last_brace != -1:\n",
    "            candidate = text[: last_brace + 1]\n",
    "            try:\n",
    "                return json.loads(candidate)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "        raise ValueError(f\"❌ Invalid JSON returned by model:\\n{text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30582c6",
   "metadata": {},
   "source": [
    "## 2. Configuration and Client Factory\n",
    "This cell defines:\n",
    "- `ClassificationConfig`: all configuration for a classification run.\n",
    "- `create_client()`: creates either an Azure OpenAI or OpenAI client.\n",
    "\n",
    "**Instructions:**\n",
    "- Set your provider to `'azure'` or `'openai'` when creating the config later.\n",
    "- For Azure, make sure environment variables `AZURE_OPENAI_API_KEY` and `AZURE_OPENAI_ENDPOINT` are set.\n",
    "- For OpenAI, set `OPENAI_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8cba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClassificationConfig:\n",
    "    \"\"\"Configuration for the classification pipeline.\"\"\"\n",
    "\n",
    "    # \"openai\" or \"azure\"\n",
    "    provider: str = \"azure\"\n",
    "\n",
    "    # OpenAI model name or Azure deployment name\n",
    "    model: str = \"gpt-4o-Global-Standard\"  # For Azure: deployment name\n",
    "\n",
    "    # System prompt (set to BASE_SYSTEM_PROMPT later)\n",
    "    system_prompt: str = \"\"\n",
    "\n",
    "    # Columns from your file used as input to the model\n",
    "    input_columns: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Columns that the model must output\n",
    "    output_columns: List[str] = field(default_factory=list)\n",
    "\n",
    "    # Optional per-column predefined categories\n",
    "    # Example:\n",
    "    # {\n",
    "    #   \"Plausible\": [\n",
    "    #       {\"name\": \"Yes\", \"definition\": \"...\"},\n",
    "    #       {\"name\": \"No\",  \"definition\": \"...\"}\n",
    "    #   ],\n",
    "    #   \"RiskLevel\": [\n",
    "    #       {\"name\": \"Low\", \"definition\": \"...\"},\n",
    "    #       {\"name\": \"High\", \"definition\": \"...\"}\n",
    "    #   ]\n",
    "    # }\n",
    "    per_column_predefined_categories: Dict[str, List[Dict[str, Any]]] = field(\n",
    "        default_factory=dict\n",
    "    )\n",
    "\n",
    "    # Optional business / domain context (project-specific)\n",
    "    business_context: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    # Model parameters\n",
    "    temperature: float = 0.0\n",
    "    max_output_tokens: Optional[int] = 2000\n",
    "\n",
    "    # How many rows per API call\n",
    "    batch_size: int = 10\n",
    "\n",
    "\n",
    "def create_client(config: ClassificationConfig):\n",
    "    \"\"\"Create a client for either Azure OpenAI or OpenAI.\"\"\"\n",
    "    if config.provider.lower() == \"azure\":\n",
    "        # Azure OpenAI (old-style chat completions)\n",
    "        return AzureOpenAI(\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),  # e.g. \"https://<resource>.openai.azure.com\"\n",
    "            api_version=\"2024-05-01-preview\",\n",
    "        )\n",
    "    else:\n",
    "        # Normal OpenAI\n",
    "        return OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d96904",
   "metadata": {},
   "source": [
    "## 3. RowClassifier (Batch Classification Logic)\n",
    "This class:\n",
    "- Creates the API client\n",
    "- Builds the JSON payload for a batch of rows\n",
    "- Calls the model using `chat.completions.create`\n",
    "- Parses the JSON response\n",
    "- Tracks categories per output column for reuse\n",
    "\n",
    "**Instructions:**\n",
    "- You usually don't need to modify this unless you want to change how batching works.\n",
    "- Focus on editing `ClassificationConfig` and `BASE_SYSTEM_PROMPT` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowClassifier:\n",
    "    def __init__(self, config: ClassificationConfig):\n",
    "        self.config = config\n",
    "        self.client = create_client(config)\n",
    "\n",
    "        # Per-column category tracking (for reuse)\n",
    "        # Example:\n",
    "        # {\n",
    "        #   \"Plausible\": {\"Yes\", \"No\"},\n",
    "        #   \"RiskLevel\": {\"Low\", \"Medium\", \"High\"}\n",
    "        # }\n",
    "        self.seen_categories_per_col: Dict[str, Set[str]] = {}\n",
    "\n",
    "        # Initialize from per_column_predefined_categories\n",
    "        for col, cats in self.config.per_column_predefined_categories.items():\n",
    "            self.seen_categories_per_col[col] = {\n",
    "                c[\"name\"] for c in cats if \"name\" in c\n",
    "            }\n",
    "\n",
    "    def _build_batch_payload(self, batch_rows: List[tuple]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Build the JSON payload for a batch of rows.\n",
    "\n",
    "        batch_rows: list of (row_index, row_series)\n",
    "        Returns a dict that will be serialized to JSON and sent to the model.\n",
    "        \"\"\"\n",
    "        rows_payload = []\n",
    "        for idx, row in batch_rows:\n",
    "            row_data = {\n",
    "                col: (\"\" if pd.isna(row.get(col)) else str(row.get(col)))\n",
    "                for col in self.config.input_columns\n",
    "            }\n",
    "            rows_payload.append(\n",
    "                {\n",
    "                    \"row_index\": int(idx),\n",
    "                    \"row_data\": row_data,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        payload = {\n",
    "            \"input_columns\": self.config.input_columns,\n",
    "            \"output_columns\": self.config.output_columns,\n",
    "            \"rows\": rows_payload,\n",
    "            # Optional per-column predefined categories\n",
    "            \"per_column_predefined_categories\": self.config.per_column_predefined_categories,\n",
    "            # Categories already used per column, for reuse and consistency\n",
    "            \"previous_categories_per_column\": {\n",
    "                col: sorted(list(vals))\n",
    "                for col, vals in self.seen_categories_per_col.items()\n",
    "            },\n",
    "            \"business_context\": self.config.business_context,\n",
    "        }\n",
    "        return payload\n",
    "\n",
    "    def classify_batch(self, batch_rows: List[tuple]) -> Dict[int, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Classify a batch of rows in a single API call using chat.completions.\n",
    "\n",
    "        Returns:\n",
    "            dict: {row_index: outputs_dict, ...}\n",
    "        \"\"\"\n",
    "        if not batch_rows:\n",
    "            return {}\n",
    "\n",
    "        payload = self._build_batch_payload(batch_rows)\n",
    "\n",
    "        # Call chat completions (old-style SDK usage)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.config.model,  # Azure: deployment name\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.config.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(payload, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=self.config.temperature,\n",
    "            max_tokens=self.config.max_output_tokens,\n",
    "        )\n",
    "\n",
    "        # Get first choice text\n",
    "        raw = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Parse JSON (robust against minor formatting issues)\n",
    "        parsed = clean_and_parse_json(raw)\n",
    "\n",
    "        if \"rows\" not in parsed or not isinstance(parsed[\"rows\"], list):\n",
    "            raise ValueError(f\"❌ JSON must contain 'rows' list:\\n{parsed}\")\n",
    "\n",
    "        result_by_index: Dict[int, Dict[str, Any]] = {}\n",
    "\n",
    "        for item in parsed[\"rows\"]:\n",
    "            if (\n",
    "                not isinstance(item, dict)\n",
    "                or \"row_index\" not in item\n",
    "                or \"outputs\" not in item\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    f\"❌ Each item must have 'row_index' and 'outputs':\\n{item}\"\n",
    "                )\n",
    "\n",
    "            row_index = int(item[\"row_index\"])\n",
    "            outputs = item[\"outputs\"]\n",
    "\n",
    "            # Track categories per output column for reuse\n",
    "            for col, value in outputs.items():\n",
    "                if not value:\n",
    "                    continue\n",
    "                if col not in self.seen_categories_per_col:\n",
    "                    self.seen_categories_per_col[col] = set()\n",
    "                self.seen_categories_per_col[col].add(str(value))\n",
    "\n",
    "            result_by_index[row_index] = outputs\n",
    "\n",
    "        return result_by_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0848325",
   "metadata": {},
   "source": [
    "## 4. File-Level Pipeline (`classify_file`)\n",
    "This function:\n",
    "- Loads an Excel or CSV file\n",
    "- Verifies input columns\n",
    "- Adds missing output columns\n",
    "- Processes rows in batches using `RowClassifier`\n",
    "- Saves the classified result to a new Excel file\n",
    "\n",
    "**Instructions:**\n",
    "- You usually only change `input_path`, `output_path`, and `sheet_name` when calling this.\n",
    "- Make sure your input file exists and has the input columns you configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_file(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    config: ClassificationConfig,\n",
    "    sheet_name: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    - Loads Excel or CSV\n",
    "    - Runs model classification in batches\n",
    "    - Writes Excel file with output columns added\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(input_path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xls\"]:\n",
    "        df = pd.read_excel(input_path, sheet_name=sheet_name)\n",
    "    elif ext == \".csv\":\n",
    "        df = pd.read_csv(input_path)\n",
    "    else:\n",
    "        raise ValueError(\"❌ Unsupported file format. Use .xlsx, .xls, or .csv\")\n",
    "\n",
    "    # Ensure input columns exist\n",
    "    for col in config.input_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"❌ Missing input column in file: {col}\")\n",
    "\n",
    "    # Create output columns if not already present\n",
    "    for col in config.output_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    classifier = RowClassifier(config)\n",
    "    indices = list(df.index)\n",
    "\n",
    "    # Process in batches\n",
    "    for start in range(0, len(indices), config.batch_size):\n",
    "        end = start + config.batch_size\n",
    "        batch_indices = indices[start:end]\n",
    "        batch_rows = [(idx, df.loc[idx]) for idx in batch_indices]\n",
    "\n",
    "        try:\n",
    "            outputs_by_index = classifier.classify_batch(batch_rows)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error in batch {start}–{end}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Fill DataFrame with outputs\n",
    "        for idx in batch_indices:\n",
    "            if idx not in outputs_by_index:\n",
    "                continue\n",
    "            outputs = outputs_by_index[idx]\n",
    "            for col in config.output_columns:\n",
    "                if col in outputs:\n",
    "                    df.at[idx, col] = outputs[col]\n",
    "\n",
    "        print(f\"✅ Processed rows {start}–{min(end, len(indices)) - 1}\")\n",
    "\n",
    "    # Save result\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"✅ Classification complete. Output saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8d825",
   "metadata": {},
   "source": [
    "## 5. Optimized System Prompt\n",
    "This is the **core brain** of the classifier.\n",
    "- It explains how to treat input/output columns.\n",
    "- It defines behavior for categorical vs freetext output columns.\n",
    "- It enforces strict JSON output.\n",
    "\n",
    "**Instructions:**\n",
    "- You can adapt business logic here (e.g. domain rules, logistics rules).\n",
    "- Keep the overall structure and formatting rules intact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI engine that classifies and enriches tabular data. \n",
    "You will receive batched rows and must return STRICT JSON output for each row.\n",
    "\n",
    "---------------------------------------------------------\n",
    "INPUT YOU WILL RECEIVE\n",
    "---------------------------------------------------------\n",
    "You receive one JSON object containing:\n",
    "\n",
    "- input_columns: list of column names whose values appear in each row_data.\n",
    "- output_columns: list of column names you must generate.\n",
    "- rows: list of items:\n",
    "    {\n",
    "      \"row_index\": <int>,\n",
    "      \"row_data\": { \"<col>\": \"<value>\", ... }\n",
    "    }\n",
    "- per_column_predefined_categories (optional):\n",
    "    {\n",
    "      \"<output_col>\": [\n",
    "        {\"name\": \"<category>\", \"definition\": \"<meaning>\"},\n",
    "        ...\n",
    "      ]\n",
    "    }\n",
    "- previous_categories_per_column (optional):\n",
    "    {\n",
    "      \"<output_col>\": [\"existing\", \"categories\", ...]\n",
    "    }\n",
    "- business_context (optional):\n",
    "    Domain rules, logistics logic, constraints, or definitions.\n",
    "    These override generic assumptions.\n",
    "\n",
    "ALWAYS:\n",
    "- Use ALL input columns together to understand meaning.\n",
    "- Respect business_context when making decisions.\n",
    "\n",
    "---------------------------------------------------------\n",
    "HOW TO ASSIGN OUTPUT COLUMN VALUES\n",
    "---------------------------------------------------------\n",
    "\n",
    "For EACH output column C in output_columns:\n",
    "\n",
    "1) IF C has predefined categories:\n",
    "   - Treat it as a **strict categorical column**.\n",
    "   - Use ONLY the category names in per_column_predefined_categories[C].\n",
    "   - Use category **definitions** to choose the correct one.\n",
    "   - NEVER invent new categories for this column.\n",
    "   - previous_categories_per_column[C] may be referenced for consistency,\n",
    "     but predefined categories override everything.\n",
    "\n",
    "2) IF C does NOT have predefined categories:\n",
    "   - Determine whether it is **categorical** or **freetext** based on its name and purpose.\n",
    "\n",
    "   A) FREETEXT COLUMNS (e.g., \"Reason\", \"Comment\", \"Explanation\", \"Description\"):\n",
    "      - MUST generate fresh, natural-language text for this row.\n",
    "      - Completely IGNORE previous_categories_per_column[C].\n",
    "      - Write clear, factual, row-specific explanations.\n",
    "\n",
    "   B) CATEGORICAL COLUMNS WITHOUT A PREDEFINED LIST:\n",
    "      - You MAY create new categories when needed.\n",
    "      - BEFORE creating new categories:\n",
    "          - Check previous_categories_per_column[C] and reuse an existing one if it fits.\n",
    "      - Avoid producing near-duplicate categories.\n",
    "      - Ensure consistency across similar rows.\n",
    "\n",
    "---------------------------------------------------------\n",
    "REASONING REQUIREMENTS\n",
    "---------------------------------------------------------\n",
    "- Use definition-based comparison for predefined categories.\n",
    "- Use contextual reasoning for other categorical columns.\n",
    "- Consider ALL input columns together.\n",
    "- Apply business_context when interpreting thresholds, ranges, or domain-specific rules.\n",
    "- Prefer stable, repeatable decisions over creative ones.\n",
    "\n",
    "---------------------------------------------------------\n",
    "STRICT OUTPUT FORMAT (NO EXCEPTIONS)\n",
    "---------------------------------------------------------\n",
    "You must return EXACTLY the following JSON structure:\n",
    "\n",
    "{\n",
    "  \"rows\": [\n",
    "    {\n",
    "      \"row_index\": <same integer>,\n",
    "      \"outputs\": {\n",
    "        \"<OutputColumn1>\": \"<value>\",\n",
    "        \"<OutputColumn2>\": \"<value>\",\n",
    "        ...\n",
    "      }\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "RULES:\n",
    "- DO NOT add any text outside the JSON object.\n",
    "- DO NOT use markdown or code fences.\n",
    "- DO NOT add keys that are not in output_columns.\n",
    "- DO NOT omit any output column.\n",
    "- ALL values must be strings.\n",
    "- row_index MUST match the input row_index.\n",
    "\n",
    "---------------------------------------------------------\n",
    "BEHAVIORAL RULES\n",
    "---------------------------------------------------------\n",
    "- No hallucinations.\n",
    "- No renaming of columns or categories.\n",
    "- No invented structure.\n",
    "- For categorical columns: consistency is mandatory.\n",
    "- For freetext columns: clarity and accuracy are mandatory.\n",
    "- When unsure, choose the most contextually justified category.\n",
    "\n",
    "Your job: analyze each row → produce valid JSON exactly as required.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da803b83",
   "metadata": {},
   "source": [
    "## 6. Example Usage\n",
    "This cell shows how to configure and run the classifier on a real file.\n",
    "\n",
    "**Steps to use:**\n",
    "1. Make sure your input Excel/CSV file exists (update the path as needed).\n",
    "2. Adjust `input_columns` and `output_columns` to match your file.\n",
    "3. Optionally define `per_column_predefined_categories` and `business_context`.\n",
    "4. Run the cell to generate the output Excel file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example configuration for a logistics plausibility check\n",
    "    config = ClassificationConfig(\n",
    "        provider=\"azure\",              # or \"openai\"\n",
    "        model=\"gpt-4o-rpe\",            # Azure deployment name or OpenAI model name\n",
    "        system_prompt=BASE_SYSTEM_PROMPT,\n",
    "\n",
    "        # Input columns from your Excel/CSV file\n",
    "        input_columns=[\"Length\", \"Width\", \"Height\", \"Weight\"],\n",
    "\n",
    "        # Output columns to be generated by the model\n",
    "        output_columns=[\"Plausible\", \"RiskLevel\", \"Reason\"],\n",
    "\n",
    "        # Optional predefined categories per output column\n",
    "        per_column_predefined_categories={\n",
    "            \"Plausible\": [\n",
    "                {\"name\": \"Yes\", \"definition\": \"Item is suitable for sea shipping.\"},\n",
    "                {\"name\": \"No\",  \"definition\": \"Item is not suitable for sea shipping.\"}\n",
    "            ],\n",
    "            \"RiskLevel\": [\n",
    "                {\"name\": \"Low\",    \"definition\": \"Fits comfortably within standard limits.\"},\n",
    "                {\"name\": \"Medium\", \"definition\": \"Near standard limits.\"},\n",
    "                {\"name\": \"High\",   \"definition\": \"Exceeds limits or requires special handling.\"}\n",
    "            ]\n",
    "            # \"Reason\" has no predefined categories → treated as freetext\n",
    "        },\n",
    "\n",
    "        # Optional business knowledge\n",
    "        business_context={\n",
    "            \"company\": \"Siemens Energy\",\n",
    "            \"shipping_rules\": {\n",
    "                \"max_length\": 400,\n",
    "                \"max_width\": 240,\n",
    "                \"max_height\": 220,\n",
    "                \"max_weight\": 5000\n",
    "            }\n",
    "        },\n",
    "\n",
    "        temperature=0.0,\n",
    "        max_output_tokens=1500,\n",
    "        batch_size=10,\n",
    "    )\n",
    "\n",
    "    # Run classification on an input file\n",
    "    classify_file(\n",
    "        input_path=\"input_data.xlsx\",      # change to your real file path\n",
    "        output_path=\"classified_output.xlsx\",\n",
    "        config=config,\n",
    "        sheet_name=\"Sheet1\",               # or None for CSV\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
